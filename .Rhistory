data=batchesList[[i]])))
}
names(batches)<-names(batchesList)[dataset$accumulate.batches:length(batchesList)]
return(batches)
}
get.batches.list<-function(dataset){
#parameters
data<-dataset$data
granularity<-dataset$granularity
dateColumnLabel<-dataset$dateColumnLabel
dateFormat<-dataset$dateFormat
if(!dataset$flagValidation){
return(split(
data, #[,!(names(data) %in% c(dateColumnLabel))],
format(data[dateColumnLabel], dateFormat)
))
}
b.list <- split(data, (seq(nrow(data))-1) %/% granularity)
names(b.list)<-seq(granularity, nrow(data), by=granularity)/1000
return(b.list)
}
get.firstBatch<-function(batchesList, dataset){
firstBatch<-data.frame()
for(a in 1:dataset$accumulate.batches){
firstBatch<-rbind(firstBatch, batchesList[[a]])
}
return(list(batchID=1,
batchName=names(batchesList)[dataset$accumulate.batches],
dateStart=ifelse(dataset$flagValidation,
1,
get.firstDay(names(batchesList)[1]), dataset$dateFormat),
dateEnd=ifelse(dataset$flagValidation,
dataset$granularity*dataset$accumulate.batches,
get.lastDay(names(batchesList)[dataset$accumulate.batches]), dataset$dateFormat),
data=firstBatch))
}
get.firstDay<-function(dateString, dateFormat){
flagYearBatches<-ifelse(dateFormat=="%Y", TRUE, FALSE)
newdate<-as.Date(paste0(dateString, ifelse(flagYearBatches,"/01/01","/01")))
return(format(newdate, ifelse(flagYearBatches,"%Y/%m","%Y/%m/%d")))
}
get.lastDay<-function(dateString, dateFormat){
flagYearBatches<-ifelse(dateFormat=="%Y", TRUE, FALSE)
first.day<-as.Date(paste0(dateString, ifelse(flagYearBatches,"/01/01","/01")))
newdate<-ceiling_date(first.day, "month")-1
return(format(newdate, ifelse(flagYearBatches,"%Y/%m", "%Y/%m/%d")))
}
get.batchName<-function(batches,i){
return(names(batches)[i])
}
dataset$batches<-set.batches(dataset)
View(dataset)
get.firstBatch<-function(batchesList, dataset){
firstBatch<-data.frame()
for(a in 1:dataset$accumulate.batches){
firstBatch<-rbind(firstBatch, batchesList[[a]])
}
return(list(batchID=1,
batchName=names(batchesList)[dataset$accumulate.batches],
dateStart=ifelse(dataset$flagValidation,
1,
get.firstDay(names(batchesList)[1], dataset$dateFormat)),
dateEnd=ifelse(dataset$flagValidation,
dataset$granularity*dataset$accumulate.batches,
get.lastDay(names(batchesList)[dataset$accumulate.batches]), dataset$dateFormat),
data=firstBatch))
}
dataset$batches<-set.batches(dataset)
batchesList<- get.batches.list(dataset)
View(batchesList)
batches<-list(get.firstBatch(batchesList, dataset))
firstBatch<-data.frame()
for(a in 1:dataset$accumulate.batches){
firstBatch<-rbind(firstBatch, batchesList[[a]])
}
View(firstBatch)
names(batchesList)[dataset$accumulate.batches]
get.firstBatch<-function(batchesList, dataset){
firstBatch<-data.frame()
for(a in 1:dataset$accumulate.batches){
firstBatch<-rbind(firstBatch, batchesList[[a]])
}
return(list(batchID=1,
batchName=names(batchesList)[dataset$accumulate.batches],
dateStart=ifelse(dataset$flagValidation,
1,
get.firstDay(names(batchesList)[1], dataset$dateFormat)),
dateEnd=ifelse(dataset$flagValidation,
dataset$granularity*dataset$accumulate.batches,
get.lastDay(names(batchesList)[dataset$accumulate.batches], dataset$dateFormat)),
data=firstBatch))
}
batches<-list(get.firstBatch(batchesList, dataset))
View(batches)
dataset$batches<-NULL
View(batches)
View(dataset)
batchesList<- get.batches.list(dataset)
View(batchesList)
batchesList[["60"]]
View(dataset)
dataset$data[55001,]
batches<-list(get.firstBatch(batchesList, dataset))
View(dataset)
dataset[["data"]]
View(dataset)
dataset[["data"]]
read.data <- function(datasetLabel, origin=NULL){
switch (datasetLabel,
"COVID" = {df <- read.CovidData(origin)},
"CVD" = {df <- read.CVDdata()},
{
df<-read.validationData(datasetLabel) #abrupt or gradual
}
)
return(df)
}
set.parameters<-function(datasetLabel){
if(datasetLabel=="COVID"){
granularity <- "month"          #batch window (day/month/year)
dateFormat <- "%Y/%m"           #format of the time column (%Y , %Y-%m)
dateColumnLabel <- "covid_dt"   #meaningful columns
classColumnLabel <- "Death"
accumulate.batches <-4          #how many batches to accumulate
flagValidation <- FALSE         #is a validation dataset i.e. simulated
} else if(datasetLabel=="CVD"){
granularity <- "year"
dateFormat <- "%Y"
dateColumnLabel <- "Diagnosis.Date"
classColumnLabel <- "death"
accumulate.batches <-2
flagValidation <- FALSE
} else{
granularity <- 5000
dateFormat <- NaN
dateColumnLabel <- NaN
classColumnLabel <- "class"
accumulate.batches <-1
flagValidation <- TRUE
}
return(list(name=datasetLabel,
granularity=granularity,
dateFormat=dateFormat,
dateColumnLabel=dateColumnLabel,
classColumnLabel=classColumnLabel,
accumulate.batches=accumulate.batches,
flagValidation=flagValidation)
)
}
##########
# HANDLE BATCHES
##########
set.batches<-function(dataset){
batchesList<- get.batches.list(dataset)
batches<-list(get.firstBatch(batchesList, dataset))
for(i in (dataset$accumulate.batches+1):length(batchesList)){
batches<-append(batches, list(list(batchID=length(batches)+1,
batchName=names(batchesList)[i],
dateStart=ifelse(dataset$flagValidation, ((i-1)*dataset$granularity)+1, get.firstDay(names(batchesList)[i])),
dateEnd=ifelse(dataset$flagValidation, i*dataset$granularity, get.lastDay(names(batchesList)[i])),
data=batchesList[[i]])))
}
names(batches)<-names(batchesList)[dataset$accumulate.batches:length(batchesList)]
return(batches)
}
get.batches.list<-function(dataset){
#parameters
data<-dataset$data
granularity<-dataset$granularity
dateColumnLabel<-dataset$dateColumnLabel
dateFormat<-dataset$dateFormat
if(!dataset$flagValidation){
return(split(
data, #[,!(names(data) %in% c(dateColumnLabel))],
format(data[dateColumnLabel], dateFormat)
))
}
b.list <- split(data, (seq(nrow(data))-1) %/% granularity)
names(b.list)<-seq(granularity, nrow(data), by=granularity)/1000
return(b.list)
}
get.firstBatch<-function(batchesList, dataset){
firstBatch<-data.frame()
for(a in 1:dataset$accumulate.batches){
firstBatch<-rbind(firstBatch, batchesList[[a]])
}
return(list(batchID=1,
batchName=names(batchesList)[dataset$accumulate.batches],
dateStart=ifelse(dataset$flagValidation,
1,
get.firstDay(names(batchesList)[1], dataset$dateFormat)),
dateEnd=ifelse(dataset$flagValidation,
dataset$granularity*dataset$accumulate.batches,
get.lastDay(names(batchesList)[dataset$accumulate.batches], dataset$dateFormat)),
data=firstBatch))
}
get.firstDay<-function(dateString, dateFormat){
flagYearBatches<-ifelse(dateFormat=="%Y", TRUE, FALSE)
newdate<-as.Date(paste0(dateString, ifelse(flagYearBatches,"/01/01","/01")))
return(format(newdate, ifelse(flagYearBatches,"%Y/%m","%Y/%m/%d")))
}
get.lastDay<-function(dateString, dateFormat){
flagYearBatches<-ifelse(dateFormat=="%Y", TRUE, FALSE)
first.day<-as.Date(paste0(dateString, ifelse(flagYearBatches,"/01/01","/01")))
newdate<-ceiling_date(first.day, "month")-1
return(format(newdate, ifelse(flagYearBatches,"%Y/%m", "%Y/%m/%d")))
}
get.batchName<-function(batches,i){
return(names(batches)[i])
}
set.dataset<-function(datasetLabel){
dataset<-set.parameters(datasetLabel)
dataset$data<-read.data(datasetLabel)
dataset$batches<-set.batches(dataset)
return(dataset)
}
Dataset<-set.dataset("Abrupt")
read.COVIDdata <- function(origin) {
switch (origin,
"SYNTHETIC" = {
path <-
"Data/Data/2021-04-14_Synthetic_Of_2021-04-14_GTNegAndPositives.csv"
},
"REAL" = {
path <- "Z:/PioneerCOVIDReal/REAL_COVID_CLEANED.csv"
},
{
stop("ORIGIN NOT FOUND!")
})
#data reader
df <- read.csv(path, header = TRUE)
df <-
subset(df,
df$Gender != "U" & df$Gender != "I" &
df$imd_5 != -1 & df$rurban != -1)
#date column
df$covid_dt <-
as.Date(df$covid_dt,
format = ifelse(origin == "REAL", "%d/%m/%Y", "%Y-%m-%d"))
#select meaningful columns
switch(origin,
"SYNTHETIC" = {
df <- df %>%
select(-X,-patid,-age,-DeathDate,-Tamiflu_rx) %>%
filter(covid_dt <= as.Date('2021-04-30'))
#filter(covid_dt >= as.Date('2020-03-01') & covid_dt <= as.Date('2021-04-30'))
#list unbalanced variables
unbalanced.var <-
c("SAMA_rx", "Chloro_Hydroxychloro_rx", "isPositive")
},
"REAL" = {
df <- df %>%
select(-ARB_rx,
-ICSSABA_rx,
-AminoTheophy_rx,
-SAMA_rx,
-Tamiflu_rx) %>%
filter(covid_dt <= as.Date('2021-10-30'))
#list unbalanced variables
unbalanced.var <-
c("AminoTheophy_rx", "LearningDisability")
},
{
stop("ORIGIN NOT FOUND!")
})
factors.cols <- colnames(select(df, -covid_dt))
df <- df %>% mutate_at(factors.cols, factor) %>% arrange(covid_dt)
return(df)
}
read.CVDdata <- function() {
df <- read.csv("Data/Data/rsample.csv",
header = TRUE)
#check datatypes
#set the class as 0/1 factor
df$death <- ifelse(df$death == "yes", 1, 0)
df$treated.hypertension <-
ifelse(df$treated.hypertension == "yes", 1, 0)
##1) factors
factors.cols <-
colnames(select(
df,
-age,
-BMI,
-SBP,
-DBP,
-cholesterol.ratio,
-Diagnosis.Date
))
df <- df %>% mutate_at(factors.cols, factor)
##2) dates format
df$Diagnosis.Date <- as.Date(df$Diagnosis.Date, "%d/%m/%Y")
##3) continuous data
continuous.cols <-
colnames(select(df, age, BMI, SBP, DBP, cholesterol.ratio))
df <- df %>% mutate_at(continuous.cols, as.numeric)
#arrange by the date
df <- df %>%
arrange(Diagnosis.Date)
#DISCRETIZATION
#df[,continuous.cols] <- discretize(df[, continuous.cols], breaks = c(20,20,20,10,12), method='interval')
return(df)
}
read.validationData <- function(datasetLabel) {
switch (
datasetLabel,
"Abrupt" = {
file.path <- "Data/Data/abrupt_dataset_orig.arff"
},
"Gradual_W10K" = {
file.path <- "Data/Data/gradualW10K_dataset_orig.arff"
},
"Gradual_W20K" = {
file.path <- "Data/Data/gradualW20K_dataset_orig.arff"
},
)
df <- read.csv(file.path, header = FALSE, comment.char = "@")
#delete NAN column
df$V11 <- NULL
#rename columns
names(df) <-
c(
"salary",
"commission",
"age",
"elevel",
"car",
"zipcode",
"hvalue",
"hyears",
"loan",
"class"
)
#refactor datatype
df$class <- ifelse(df$class == "groupB", 1, 0)
factors.col <- c("elevel", "car", "zipcode", "class")
df[, factors.col] <- lapply(df[, factors.col], factor)
return(df)
}
Dataset<-set.dataset("Abrupt")
View(Dataset)
#libraries
cat("Libraries loading...\n")
source("libraries.R")
#functions
cat("Functions loading...\n")
source("functions.R")
# Dataset initialisation
# Dataset<-initialise_dataset("Gradual_W10K")
Dataset<-initialise_dataset("COVID", "SYNTHETIC")
# Dataset initialisation
# Dataset<-initialise_dataset("Gradual_W10K")
Dataset<-initialise_dataset("COVID")
# Dataset initialisation
# Dataset<-initialise_dataset("Gradual_W10K")
Dataset<-initialise_dataset("COVID")
# Dataset initialisation
# Dataset<-initialise_dataset("Gradual_W10K")
Dataset<-initialise_dataset("COVID")
dataset_obj$name
# Dataset initialisation
# Dataset<-initialise_dataset("Gradual_W10K")
Dataset<-initialise_dataset("COVID")
# Dataset initialisation
# Dataset<-initialise_dataset("Gradual_W10K")
Dataset<-initialise_dataset("COVID-SYN")
# Dataset initialisation
# Dataset<-initialise_dataset("Gradual_W10K")
Dataset<-initialise_dataset("COVID-SYN")
dir
source("functions.R")
# Dataset initialisation
# Dataset<-initialise_dataset("Gradual_W10K")
Dataset<-initialise_dataset("COVID-SYN")
# Dataset initialisation
# Dataset<-initialise_dataset("Gradual_W10K")
Dataset<-initialise_dataset("COVID-SYN")
#library to deal with dates (covid and cvd data)
install.packages('lubridate')
install.packages("lubridate")
library()
installed.packages()
#library to deal with dates (covid and cvd data)
library(lubridate)
# Dataset initialisation
# Dataset<-initialise_dataset("Gradual_W10K")
Dataset<-initialise_dataset("COVID-SYN")
# Dataset initialisation
# Dataset<-initialise_dataset("Gradual_W10K")
Dataset<-initialise_dataset("COVID-SYN")
split(
data, #[,!(names(data) %in% c(dateColumnLabel))],
format(data[date_column_label], date_format)
)
length(batches)+1
name=names(raw_batches)[i]
get_first_day(names(raw_batches)[i])
names(raw_batches)[i]
source("functions.R")
source("libraries.R")
# Dataset initialisation
# Dataset<-initialise_dataset("Gradual_W10K")
Dataset<-initialise_dataset("COVID-SYN")
View(Dataset)
# Train regression models
training <-compute_performance_shifts(Dataset, "training")
training<-append_drift_detection_metrics(Dataset, "training", training)
source("libraries.R")
source("functions.R")
# Dataset initialisation
# Dataset<-initialise_dataset("Gradual_W10K")
Dataset<-initialise_dataset("COVID-SYN")
# Train regression models
training <-compute_performance_shifts(Dataset, "training")
summary(dataset_obj)
dataset_obj[[data_group]][[i]]
dataset_obj[[data_group]][[i]]
dataset_obj$training
performance
training
training<-append_drift_detection_metrics(Dataset, "training", training)
training<-append_drift_detection_metrics(Dataset, "training", training)
source("functions.R")
training<-append_drift_detection_metrics(Dataset, "training", training)
training<-append_drift_detection_metrics(Dataset, "training", training)
training
source("functions.R")
source("functions.R")
training$discrimination_error <- NULL
training$avr_prob_score <- NULL
training
training<-append_drift_detection_metrics(Dataset, "training", training)
training
debugSource("C:/Users/yleni/OneDrive - Brunel University London/Desktop/Brunel University/Brunel University/2022 - Predicting performance drops/Implementation/functions.R")
training
training$discrimination_error <- NULL
training$avr_prob_score <- NULL
training<-append_drift_detection_metrics(Dataset, "training", training)
target$Death
source$Death
new_data$Death
table(new_data$Death)
summary(new_data$Death)
head(new_data, 30)
print(head(new_data, 30))
head(new_data, 30)
head(new_data$Death, 30)
head(new_data$Death, 90)
predicted_class
summary(predicted_class)
source("functions.R")
training
training<-append_drift_detection_metrics(Dataset, "training", training)
training<-append_drift_detection_metrics(Dataset, "training", training)
head(union$Death)
## 1.DISCRIMINATION ERROR
compute_discrimination_error<-function(source, target, class_column_label){
source[[class_column_label]]<-as.factor(1)
target[[class_column_label]]<-as.factor(0)
#data sets balancing (both with the same amount of data)
if(nrow(source) > nrow(target)){
source<-sample_n(source, nrow(target))
} else {
target<-sample_n(target, nrow(source))
}
#to balance the data ans shuffle the order of the rows
union<-union_all(source,target) %>% sample_n(n(), replace = FALSE)
training<-sample_frac(union, 0.7)
test<-setdiff(union,training)
model<-train_model(training, class_column_label)
error <- 1 - compute_model_performance(model, test, class_column_label)
return(error)
}
debugSource("C:/Users/yleni/OneDrive - Brunel University London/Desktop/Brunel University/Brunel University/2022 - Predicting performance drops/Implementation/functions.R")
training<-append_drift_detection_metrics(Dataset, "training", training)
head(union$Death)
# Create a sample data frame
data_frame <- data.frame(
ID = 1:5,
Name = c("Alice", "Bob", "Charlie", "David", "Eve")
)
# Create a sample data frame
data_frame <- data.frame(
ID = 1:5,
Name = c("Alice", "Bob", "Charlie", "David", "Eve")
)
View(data_frame)
# Shuffle the rows using dplyr
shuffled_data_frame <- data_frame %>% sample_n(n(), replace = FALSE)
View(shuffled_data_frame)
rm(data_frame)
rm(shuffled_data_frame)
training
training$DE_shift<-c(0, diff(training$discrimination_error))
training$APS_shift<-c(0, diff(training$avr_prob_score))
# Plot regression models
drift_metrics<-c("discrimination_error", "avr_prob_score")
for(metric in drift_metrics){
plot_regression_model(training, metric)
}
## Evaluate algorithm
test<-compute_performance_shifts(Dataset, "test")
test<-append_drift_detection_metrics(Dataset, "test", test)
test
test$perf_shift_pred_baseline<-compute_mean_performance_shift(training)
test <-append_performance_shifts_pred(training, test, drift_metrics)
# Plot results
plot_drift_detection(training, drift_metrics, "metrics")
plot_drift_detection(test, drift_metrics, "metrics")
plot_drift_detection(test, drift_metrics, "drop_prediction")
## Compute errors
errors<-compute_errors(test, drift_metrics)
print_output(errors)
